# README for s1234567_BDP_A1

## Overview
This assignment includes three tasks implemented using Python and Hadoop Streaming on AWS EMR. The tasks focus on taxi trip analysis using MapReduce programs. The code files and shell scripts are organized in the same folder, and this document provides the necessary steps to run each task.

## Prerequisites
- AWS EMR cluster set up with Hadoop and Python installed.
- Input files `Trips.txt`, `Taxis.txt` and `initialization.txt` uploaded to the HDFS `/Input/` directory.
- Hadoop Streaming JAR available at `./hadoop-streaming-3.1.4.jar`.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Task 1: Trip Classification and Fare Analysis
1. **Objective**: Classify trips into long, medium, and short categories and calculate total, max, min, and average fares for each category.
2. **How to Run**:
   - Run the following command to execute the task:
     ```bash
      chmod +x Task1-run.sh
     ./Task1-run.sh
     ```
   - The output will be saved in `/Output/Task1`.
3. **Commands to View the Output**:
   hadoop fs -ls /Output/Task1
   hadoop fs -cat /Output/Task1/part-00000
   hadoop fs -cat /Output/Task1/part-00001
   hadoop fs -cat /Output/Task1/part-00002

  **Use** : `cat Task1_output.txt` to get merged output.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


## Task 2: K-Medoid Clustering of Trips
1. **Objective**: Cluster trips based on dropoff locations using the Partitioning Around Medoids (PAM) algorithm.
2. **Important Step**: Before running the shell script, manually copy `initialization.txt` to `medoids.txt` and `medoids1.txt` by executing:

   ```
   cp initialization.txt medoids.txt
   cp initialization.txt medoids1.txt
   ```
3. **How to Run** :
  - Run the following command to execute the task:
     ```
     chmod +x Task2-run.sh
     ./Task2-run.sh
     ```
  - The output will be saved in `/Output/Task2`.

4. **Commands to View the Output**:
  hadoop fs -ls /Output/Task2
  hadoop fs -cat /Output/Task2/part-00000
  hadoop fs -cat /Output/Task2/part-00001
  hadoop fs -cat /Output/Task2/part-00002

  **Use** : `cat Task2_output.txt` to get merged output.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## Task 3: Trip Counting and Company Sorting
1. **Objective**: Count the number of trips for each taxi company and sort the companies in ascending order based on the total number of trips.

2. **How to Run**:
   - Run the following command to execute the task:
     ```bash
     chmod +x Task3-run.sh      
     ./Task3-run.sh
     ```
   - The output will be saved in `/Output/Task3`.

3. **Understanding the Output Directories**:
   - **IntermediateOutput**: This directory stores the results after the first MapReduce job, where the join operation between `Trips.txt` and `Taxis.txt` is performed, and the trip counts for each company are calculated. The files `part-00000`, `part-00001`, and `part-00002` represent the outputs from the three reducers used in this step. 
   - **FinalSortedOutput**: This directory contains the sorted results of the companies based on their total trip counts. The sorting operation is performed as the last step, and the final sorted data is saved in `part-00000`.

4. **Commands to View the Output**:
   - To list the content of the Task 3 output directory:
     ```bash
     hadoop fs -ls /Output/Task3
     ```
   - To view the intermediate results from the first MapReduce job:
     ```bash
     hadoop fs -ls /Output/Task3/IntermediateOutput
     hadoop fs -cat /Output/Task3/IntermediateOutput/part-00000
     hadoop fs -cat /Output/Task3/IntermediateOutput/part-00001
     hadoop fs -cat /Output/Task3/IntermediateOutput/part-00002
     ```
   - To view the final sorted results:
     ```bash
     hadoop fs -ls /Output/Task3/FinalSortedOutput
     hadoop fs -cat /Output/Task3/FinalSortedOutput/part-00000
     ```
   
   - **Explanation of Commands**:
     - `hadoop fs -ls /Output/Task3`: Lists the contents of the main output directory for Task 3, showing subdirectories created during the process.
     - `hadoop fs -ls /Output/Task3/IntermediateOutput`: Lists the files generated in the intermediate output stage, confirming successful execution of the first MapReduce step.
     - `hadoop fs -cat /Output/Task3/IntermediateOutput/part-0000X`: Displays the content of each part file generated by the reducers, showing the unsorted counts of trips for each taxi company. 
     - `hadoop fs -ls /Output/Task3/FinalSortedOutput`: Lists the files in the final output directory after sorting.
     - `hadoop fs -cat /Output/Task3/FinalSortedOutput/part-00000`: Displays the sorted list of taxi companies with their respective trip counts in ascending order, confirming the successful completion of the sorting operation.

  **Use** : `cat Task3_output.txt` to get merged output.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
